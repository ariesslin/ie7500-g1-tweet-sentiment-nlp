{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6f7be3",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ariesslin/ie7500-g1-tweet-sentiment-nlp/blob/main/scripts/3.%20Model_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80",
   "metadata": {
    "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80"
   },
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3. Model Development</strong></h2>\n",
    "  <p style=\"color:#333333;\">Model Selection and Preliminary Performance Testing</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bOElrufQWr4t",
   "metadata": {
    "id": "bOElrufQWr4t"
   },
   "source": [
    "## Model Development Overview: Multi-Architecture Sentiment Classification\n",
    "\n",
    "This notebook presents a comprehensive comparison of three distinct approaches to tweet sentiment classification, each representing different levels of complexity and modeling capability.\n",
    "\n",
    "### Why These Three Models?\n",
    "\n",
    "Our selection of **TF-IDF + Logistic Regression**, **Bidirectional LSTM**, and **BERT** provides a systematic progression through the evolution of NLP techniques:\n",
    "\n",
    "**1. Classical Baseline (TF-IDF + Logistic Regression)**\n",
    "- Establishes a strong, interpretable foundation using traditional feature engineering\n",
    "- Fast training and inference with excellent interpretability\n",
    "- Represents the minimum viable approach for text classification\n",
    "\n",
    "**2. Sequential Deep Learning (Bidirectional LSTM)**  \n",
    "- Captures context and word order through recurrent processing\n",
    "- Handles the sequential nature of language with memory mechanisms\n",
    "- Balances complexity with computational efficiency\n",
    "\n",
    "**3. State-of-the-Art Transformer (BERT)**\n",
    "- Leverages pre-trained contextual embeddings for nuanced understanding\n",
    "- Provides bidirectional context modeling with attention mechanisms\n",
    "- Represents current best practices in NLP\n",
    "\n",
    "### Evaluation Framework\n",
    "\n",
    "Each model will be evaluated using:\n",
    "- **Performance Metrics**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "- **Confusion Matrix Analysis**: Understanding error patterns\n",
    "- **Computational Efficiency**: Training time and resource requirements\n",
    "- **Qualitative Assessment**: Manual inference checks on simple examples\n",
    "\n",
    "### Detailed Implementation Notebooks\n",
    "\n",
    "ðŸ““ **[3a-Logistic-Regression.ipynb](./3a-Logistic-Regression.ipynb)** - Complete TF-IDF baseline with hyperparameter tuning and error analysis\n",
    "\n",
    "ðŸ““ **[3b-LSTM.ipynb](./3b-LSTM.ipynb)** - Bidirectional LSTM with Word2Vec embeddings and comprehensive evaluation\n",
    "\n",
    "ðŸ““ **[3c-BERT.ipynb](./3c-BERT.ipynb)** - DistilBERT fine-tuning for sentiment classification\n",
    "\n",
    "This multi-model approach ensures we select the optimal balance between performance, interpretability, and computational efficiency for tweet sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6",
   "metadata": {
    "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6"
   },
   "source": [
    "### Project Setup and Data Loading\n",
    "\n",
    "Before diving into model-specific implementations, we establish a common foundation for all models using the preprocessed Sentiment140 dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iYXzu-fYlmTw",
   "metadata": {
    "id": "iYXzu-fYlmTw",
    "outputId": "6e07bc59-5815-438e-cd7a-e3074b1a1882"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/courses/IE7500.202550/shared/conda_env_1/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "2025-06-25 06:22:01.589101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750846921.609873  421387 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750846921.616255  421387 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750846921.633074  421387 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750846921.633091  421387 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750846921.633093  421387 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750846921.633095  421387 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-25 06:22:01.638646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# importing all libraries here\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import gdown\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import zipfile\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd848f3-e91b-4f76-b584-335cb114ad1f",
   "metadata": {
    "id": "2bd848f3-e91b-4f76-b584-335cb114ad1f",
    "outputId": "06c6b329-7fc7-4cea-86f5-7d7ed60a5b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP contents: ['train_dataset.csv']\n",
      "Loaded train_dataset.csv with shape: (936194, 3)\n"
     ]
    }
   ],
   "source": [
    "# Here, we begin by loading the processed dataset\n",
    "\n",
    "# Step 1: Set the correct local path\n",
    "zip_path = \"train_dataset_comp.zip\"\n",
    "\n",
    "# Step 2: Extract only 'train_dataset.csv'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    # Check contents (optional debug)\n",
    "    print(\"ZIP contents:\", zip_ref.namelist())\n",
    "\n",
    "    # Extract only train dataset\n",
    "    zip_ref.extract(\"train_dataset.csv\")\n",
    "\n",
    "# Step 3: Load the extracted CSV\n",
    "train_df = pd.read_csv(\"train_dataset.csv\")\n",
    "print(f\"Loaded train_dataset.csv with shape: {train_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e060696-733c-42a0-a2a5-99df83302694",
   "metadata": {
    "id": "6e060696-733c-42a0-a2a5-99df83302694",
    "outputId": "b6d940c0-35c0-4d0c-d796-925de08836bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation dataset with shape: (200614, 3)\n"
     ]
    }
   ],
   "source": [
    "# We load the validation dataset\n",
    "\n",
    "val_df = pd.read_csv(\"val_dataset.csv\")\n",
    "\n",
    "print(f\"Loaded validation dataset with shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8da74-abb8-45b1-a637-e57386b02a33",
   "metadata": {
    "id": "2ba8da74-abb8-45b1-a637-e57386b02a33"
   },
   "outputs": [],
   "source": [
    "# Drop any rows where 'text' or 'target' is null\n",
    "train_df = train_df.dropna(subset=[\"text\", \"target\"])\n",
    "val_df = val_df.dropna(subset=[\"text\", \"target\"])\n",
    "\n",
    "# Replace 'MENTION' with 'mentionuser' in the 'text' column\n",
    "train_df['text'] = train_df['text'].str.replace('MENTION', 'mentionuser', regex=False)\n",
    "val_df['text'] = val_df['text'].str.replace('MENTION', 'mentionuser', regex=False)\n",
    "\n",
    "# Keep only text and target columns\n",
    "train_df=train_df[['text', 'target']]\n",
    "val_df=val_df[['text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677ad14-6a04-4b0a-847a-e7b1d078a718",
   "metadata": {
    "id": "0677ad14-6a04-4b0a-847a-e7b1d078a718",
    "outputId": "c24842fb-a019-4e25-8977-70740c84e8d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mentionuser good u great band yes think daught...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work glummy wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getting lost signpostfree road kildareoffaly w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good morning everyone</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kidding coursethough wish could part late nigh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  mentionuser good u great band yes think daught...       4\n",
       "1                              work glummy wednesday       0\n",
       "2  getting lost signpostfree road kildareoffaly w...       0\n",
       "3                              good morning everyone       4\n",
       "4  kidding coursethough wish could part late nigh...       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddf923-a47e-4736-8933-916407b8284e",
   "metadata": {
    "id": "43ddf923-a47e-4736-8933-916407b8284e",
    "outputId": "26c93d5a-5dfb-4a0f-fd0b-db3ff0d3c2e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mentionuser life lie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mentionuser airbender except stunning lack asi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loving god fullest</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kind sort awaketrying wake melissa go oral sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mentionuser bienvenue Ã  montreal canada great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                               mentionuser life lie       0\n",
       "1  mentionuser airbender except stunning lack asi...       0\n",
       "2                                 loving god fullest       4\n",
       "3  kind sort awaketrying wake melissa go oral sur...       0\n",
       "4  mentionuser bienvenue Ã  montreal canada great ...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fXfKl95mkCwK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXfKl95mkCwK",
    "outputId": "32004fc9-51e0-4fdb-da8a-c5d9259aaac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (936194, 2)\n",
      "Validation shape: (200614, 2)\n",
      "X_train shape: (936194,) | y_train shape: (936194,)\n",
      "X_val shape: (200614,) | y_val shape: (200614,)\n"
     ]
    }
   ],
   "source": [
    "# Now, we extract input and labels\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['target']\n",
    "X_val = val_df['text']\n",
    "y_val = val_df['target']\n",
    "\n",
    "# Final check\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"X_train shape:\", X_train.shape, \"| y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape, \"| y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82172ce7-5869-4448-8c89-3e9450c1e426",
   "metadata": {
    "id": "82172ce7-5869-4448-8c89-3e9450c1e426"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.1 Baseline Model â€“ TF-IDF + Logistic Regression</strong></h2>\n",
    "  <p style=\"color:#333333;\">Overview of TF-IDF vectorization + logistic regression baseline model.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa0c91-f0d1-44a5-82c5-789b0bd1e5e7",
   "metadata": {
    "id": "07efb030-35ad-4e70-9186-b72e6bd676ec"
   },
   "source": [
    "## Baseline Model Overview: TF-IDF + Logistic Regression\n",
    "\n",
    "The **TF-IDF + Logistic Regression** model serves as our baseline for tweet sentiment classification. This approach combines term frequency-inverse document frequency vectorization with a linear classifier to provide a strong, interpretable foundation for comparison.\n",
    "\n",
    "### Key Features of the Baseline Model:\n",
    "\n",
    "**TF-IDF Vectorization:**\n",
    "- Converts raw tweets into numerical feature vectors\n",
    "- Measures word importance relative to the entire corpus\n",
    "- Handles sparse, high-dimensional text data effectively\n",
    "- Applies L2 normalization to prevent length bias\n",
    "\n",
    "**Logistic Regression Classifier:**\n",
    "- Linear model that's fast to train and highly interpretable  \n",
    "- Provides probability estimates for sentiment predictions\n",
    "- Works well with TF-IDF's sparse feature representation\n",
    "- Enables coefficient analysis to understand word influences\n",
    "\n",
    "**Hyperparameter Optimization:**\n",
    "- Grid search across TF-IDF parameters (max_features, ngram_range, min_df, max_df)\n",
    "- Logistic regression tuning (regularization strength, class weighting)\n",
    "- 3-fold cross-validation for robust parameter selection\n",
    "- Weighted F1-score optimization for balanced performance\n",
    "\n",
    "### Performance Summary:\n",
    "- **Validation Accuracy**: ~79-78%\n",
    "- **F1 Score**: ~83-79%\n",
    "- **Strengths**: Fast, interpretable, solid baseline performance\n",
    "- **Limitations**: Context-insensitive, struggles with negation and nuance\n",
    "\n",
    "### For Detailed Implementation:\n",
    "ðŸ““ **See complete implementation, hyperparameter tuning, error analysis, and interpretability insights in [`3a-Logistic-Regression.ipynb`](./3a-Logistic-Regression.ipynb)**\n",
    "\n",
    "The detailed notebook includes:\n",
    "- Comprehensive data preprocessing and tokenization\n",
    "- GridSearchCV hyperparameter optimization with 96 parameter combinations\n",
    "- Model coefficient analysis and word importance visualization\n",
    "- Extensive error analysis on misclassified tweets\n",
    "- Discussion of model strengths and limitations\n",
    "\n",
    "This baseline establishes the performance threshold that our deep learning models (LSTM and BERT) should surpass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2c984-64e4-49a4-afab-160dfbc2a57a",
   "metadata": {
    "id": "61b2c984-64e4-49a4-afab-160dfbc2a57a"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.2 Deep Learning Model â€“ Bidirectional LSTM</strong></h2>\n",
    "  <p style=\"color:#333333;\">Bidirectional LSTM with Word2Vec embeddings for sequence-aware sentiment analysis.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d603b-9b00-41df-940e-30e8ad80c8b6",
   "metadata": {
    "id": "to08L5gzucYG"
   },
   "source": [
    "## Deep Learning Sentiment Classifier: Bidirectional LSTM with Word2Vec Embeddings\n",
    "\n",
    "The **Bidirectional LSTM + Word2Vec** model represents our deep learning approach for tweet sentiment classification, designed to capture the sequential nature of language and semantic relationships between words.\n",
    "\n",
    "### Key Features of the LSTM Model:\n",
    "\n",
    "**Word2Vec Embeddings:**\n",
    "- Dense word vectors that capture semantic relationships between words\n",
    "- Trained specifically on our tweet corpus for domain-specific representations  \n",
    "- 100-dimensional embeddings with 100% vocabulary coverage\n",
    "- Words with similar meanings positioned close together in embedding space\n",
    "\n",
    "**Bidirectional LSTM Architecture:**\n",
    "- Processes text in both forward and backward directions for complete context understanding\n",
    "- Two-layer bidirectional LSTM with 128 and 64 hidden units respectively\n",
    "- Dropout layers (0.5 and 0.3) to prevent overfitting during training\n",
    "- Captures long-term dependencies and sequential patterns in tweets\n",
    "\n",
    "**Advanced Features:**\n",
    "- Handles negation, word order, and contextual sentiment better than baseline\n",
    "- Early stopping with patience=3 to prevent overfitting\n",
    "- Nadam optimizer for improved convergence on noisy tweet data\n",
    "- Sequence padding to handle variable tweet lengths efficiently\n",
    "\n",
    "### Performance Summary:\n",
    "- **Validation Accuracy**: ~80.14%\n",
    "- **F1 Score**: ~80.15%\n",
    "- **Strengths**: Context-aware, handles sequence patterns, semantic understanding\n",
    "- **Limitations**: Computationally expensive, requires more training time\n",
    "\n",
    "### For Detailed Implementation:\n",
    "ðŸ““ **See complete implementation, Word2Vec training, architecture details, and comprehensive error analysis in [`3b-LSTM.ipynb`](./3b-LSTM.ipynb)**\n",
    "\n",
    "The detailed notebook includes:\n",
    "- Custom Word2Vec training on the tweet corpus with 90K+ vocabulary\n",
    "- Bidirectional LSTM architecture with embedding matrix construction\n",
    "- Comprehensive error analysis with specific examples from model output\n",
    "- Evidence-based insights into model strengths and limitations\n",
    "- Performance comparison with baseline model\n",
    "\n",
    "This LSTM model bridges the gap between simple linear models and sophisticated transformers, providing strong performance with interpretable sequential modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b2d45-8f4b-40cb-8728-2a50a2b8f18b",
   "metadata": {
    "id": "164b2d45-8f4b-40cb-8728-2a50a2b8f18b"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdf2dd-b6d2-4007-923c-f7825eba8057",
   "metadata": {
    "id": "aabdf2dd-b6d2-4007-923c-f7825eba8057"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.3 Transformer Model â€“ DistilBERT</strong></h2>\n",
    "  <p style=\"color:#333333;\">Fine-tuning DistilBERT for state-of-the-art contextual sentiment analysis.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b3fc6-5245-4190-b687-99612f572e5c",
   "metadata": {
    "id": "f6c94bb9-609c-486b-8f8b-65b4639e6aa1"
   },
   "source": [
    "## Transformer: DistilBERT Fine-tuning\n",
    "\n",
    "The **DistilBERT** model represents our state-of-the-art approach for tweet sentiment classification, leveraging pre-trained transformer architecture for deep contextual understanding.\n",
    "\n",
    "### Key Features of the DistilBERT Model:\n",
    "\n",
    "**Pre-trained Transformer Architecture:**\n",
    "- Distilled version of BERT with 97% of BERT's performance using 60% fewer parameters\n",
    "- Pre-trained on 16GB of text data, providing rich contextual representations\n",
    "- Bidirectional attention mechanism for complete sentence understanding\n",
    "- Fine-tuned specifically for binary sentiment classification\n",
    "\n",
    "**Advanced NLP Capabilities:**\n",
    "- Handles complex linguistic patterns like sarcasm, negation, and context-dependent sentiment\n",
    "- Understands word relationships across entire tweet sequences simultaneously\n",
    "- Processes subword tokens for better handling of informal social media language\n",
    "- Maximum sequence length of 140 tokens optimized for tweet analysis\n",
    "\n",
    "**Training Configuration:**\n",
    "- 2 epochs with learning rate of 1e-4 for effective fine-tuning\n",
    "- Batch size of 32 for efficient GPU utilization\n",
    "- Early stopping and weight decay for regularization\n",
    "- Specialized tokenizer for handling Twitter-specific language patterns\n",
    "\n",
    "### Performance Summary:\n",
    "- **Validation Accuracy**: ~81.49%\n",
    "- **Precision**: ~83.18% (highest among all models)\n",
    "- **Recall**: ~86.54%\n",
    "- **F1 Score**: ~84.83%\n",
    "- **Strengths**: Superior contextual understanding, handles complex linguistic patterns\n",
    "- **Limitations**: Computationally expensive, requires significant GPU resources\n",
    "\n",
    "### For Detailed Implementation:\n",
    "ðŸ““ **See complete implementation, fine-tuning process, optimization techniques, and comprehensive performance analysis in [`3c-BERT.ipynb`](./3c-BERT.ipynb)**\n",
    "\n",
    "The detailed notebook includes:\n",
    "- Comprehensive DistilBERT tokenization and dataset preparation\n",
    "- Complete fine-tuning implementation with Hugging Face Transformers\n",
    "- Advanced optimization techniques (mixed precision training, optimized sequence length)\n",
    "- Extensive performance analysis and error evaluation\n",
    "- Comparison with baseline and LSTM models\n",
    "\n",
    "This transformer model achieves the highest precision among all approaches, making it ideal for applications where accurate positive sentiment detection is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3b1cc-bf9a-4b07-9111-0bfadff815c7",
   "metadata": {
    "id": "21827938-c09d-4fc9-ab90-118c407f2b75"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4741c-4ee7-4cdc-896c-1cde34dd2576",
   "metadata": {
    "id": "a8a4741c-4ee7-4cdc-896c-1cde34dd2576"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>4 Model Comparison and Results Analysis </strong></h2>\n",
    "  <p style=\"color:#333333;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FOTd56MrOLlh",
   "metadata": {
    "id": "FOTd56MrOLlh"
   },
   "source": [
    "## Model Validation Results and Comparative Analysis for Tweet Sentiment Classification\n",
    "\n",
    "This section presents a detailed evaluation of three models **Logistic Regression**, **LSTM**, and **BERT** trained to classify tweet sentiments as **Negative** or **Positive**. Evaluation is based on confusion matrices, performance metrics, training dynamics, and comparative strengths and weaknesses in the context of tweet sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Logistic Regression â€” Validation Results\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy**: 79.22%\n",
    "- **Precision**: 80.04%\n",
    "- **Recall**: 86.93%\n",
    "- **F1 Score**: 83.34%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|                   | **Predicted Negative** | **Predicted Positive** |\n",
    "|-------------------|------------------------|-------------------------|\n",
    "| **Actual Negative** | 54,639                 | 26,013                  |\n",
    "| **Actual Positive** | 15,678                 | 100,000+                |\n",
    "\n",
    "**Sentiment Analysis Insights:**\n",
    "- Logistic Regression shows solid baseline performance and favors **recall**, which means it's highly sensitive to detecting **positive sentiments** in tweets.\n",
    "- However, the high number of **false positives (26,013)** suggests it struggles to distinguish **genuinely negative tweets**, often misclassifying them as positive.\n",
    "- This behavior may be due to:\n",
    "  - Over-simplification of tweet content and lack of contextual understanding.\n",
    "  - Tweets containing mixed signals (e.g., sarcasm or slang) being interpreted incorrectly.\n",
    "\n",
    "- **Use Case Suitability**:\n",
    "  - Adequate for large-scale monitoring where missing positive sentiment is riskier than mistakenly flagging negative ones (e.g., brand loyalty tracking).\n",
    "  - Not ideal where negative sentiment needs precise monitoring (e.g., social crisis detection).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. LSTM â€” Validation and Training Results\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy**: 81.17%\n",
    "- **Precision**: 81.51%\n",
    "- **Recall**: 88.60%\n",
    "- **F1 Score**: 84.91%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|                   | **Predicted Negative** | **Predicted Positive** |\n",
    "|-------------------|------------------------|-------------------------|\n",
    "| **Actual Negative** | 56,546                 | 24,106                  |\n",
    "| **Actual Positive** | 13,672                 | 106,290                 |\n",
    "\n",
    "**Training Summary:**\n",
    "- Validation accuracy and loss stabilized around epoch 6, suggesting thatâ€™s the best point to stop training.\n",
    "- Model learned rapidly early on, but overfitting started appearing after epoch 6.\n",
    "\n",
    "**Sentiment Analysis Insights:**\n",
    "- The LSTM model is especially well-suited for sentiment classification on tweets due to its ability to model **sequence dependencies**.\n",
    "- Tweets often contain **non-standard grammar**, **emojis**, and **elongations** (e.g., \"soooo goood\"), which LSTM handles more effectively than a linear model.\n",
    "- Strong performance in **both recall and precision** implies it can:\n",
    "  - Accurately detect **positive** sentiments.\n",
    "  - Avoid incorrectly labeling **negative tweets** as positive.\n",
    "\n",
    "- **Use Case Suitability**:\n",
    "  - Ideal for real-time sentiment dashboards and public opinion tracking tools.\n",
    "  - Offers a strong balance between catching enthusiastic sentiment and minimizing false optimism.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. BERT â€” Validation and Training Results\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy**: 81.49%\n",
    "- **Precision**: 83.18%\n",
    "- **Recall**: 86.54%\n",
    "- **F1 Score**: 84.83%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|                   | **Predicted Negative** | **Predicted Positive** |\n",
    "|-------------------|------------------------|-------------------------|\n",
    "| **Actual Negative** | ~60,000                | 20,991                  |\n",
    "| **Actual Positive** | 16,150                 | 100,000+                |\n",
    "\n",
    "**Training Summary:**\n",
    "- Training loss dropped smoothly from 0.4557 â†’ 0.3960 over 55,000 steps.\n",
    "- Stable and consistent learning curve, reflecting effective pretraining and fine-tuning.\n",
    "\n",
    "**Sentiment Analysis Insights:**\n",
    "- BERT outperforms all models in **precision**, meaning it is highly accurate in identifying **positive sentiment** when it makes that prediction.\n",
    "- This is crucial for tweets where subtle context (e.g., sarcasm, negation, idioms) makes sentiment ambiguous.\n",
    "- The slightly lower recall (compared to LSTM) indicates BERT may miss some **less explicit** positive tweets, possibly those that use slang or non-standard formatting unseen during fine-tuning.\n",
    "\n",
    "- **Use Case Suitability**:\n",
    "  - Excellent for **automated sentiment scoring**, content moderation, or **flagging key influencer reactions**.\n",
    "  - Especially beneficial when **false positives** (mistakenly labeling negativity as positivity) are more damaging (e.g., in reputation management or crisis alerts).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Comparative Model Analysis and Recommendations\n",
    "\n",
    "| **Metric**          | **Logistic Regression** | **LSTM**           | **BERT**           |\n",
    "|---------------------|--------------------------|--------------------|--------------------|\n",
    "| Accuracy            | 79.22%                   | 81.17%             | **81.49%**         |\n",
    "| Precision (Positive)| 80.04%                   | 81.51%             | **83.18%**         |\n",
    "| Recall (Positive)   | 86.93%                   | **88.60%**         | 86.54%             |\n",
    "| F1 Score            | 83.34%                   | **84.91%**         | 84.83%             |\n",
    "| False Positives     | 26,013                   | 24,106             | **20,991**         |\n",
    "| False Negatives     | 15,678                   | **13,672**         | 16,150             |\n",
    "\n",
    "**Overall Insights:**\n",
    "- **Logistic Regression** remains a decent choice for high-volume, low-compute environments, where identifying most of the **positive sentiments** is more critical than precision.\n",
    "- **LSTM** is the most **balanced** model, particularly effective at picking up **positive sentiment** while minimizing false detections. Its sequential modeling helps handle emotive expressions common in tweets.\n",
    "- **BERT** offers the best **precision**, indicating itâ€™s the most confident and contextually aware when assigning **positive sentiment**, but it may miss tweets that use creative or ambiguous language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZukGRi5gXirg",
   "metadata": {
    "id": "ZukGRi5gXirg"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>5 Next Steps for Model Evaluation and Selection </strong></h2>\n",
    "  <p style=\"color:#333333;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fLpTsYW6YgSg",
   "metadata": {
    "id": "fLpTsYW6YgSg"
   },
   "source": [
    "## Next Steps: Model Evaluation and Selection Plan\n",
    "\n",
    "To ensure a rigorous and unbiased model selection process for tweet sentiment classification, we will follow a structured approach that distinctly separates **model selection**, **training**, and **testing** phases. This separation ensures that each decision is grounded in fair comparison and sound generalization principles.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Evaluate All Models Using ROC-AUC**\n",
    "- For each of the three models (Logistic Regression, LSTM, BERT), compute the **ROC-AUC score** on the validation set.\n",
    "- ROC-AUC is selected because it provides a balanced view of model discrimination capability between positive and negative tweet sentiments.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Perform Manual Inference Checks**\n",
    "- Each model will be tested on two simple handcrafted examples:\n",
    "  - **Positive Sentiment**: `\"Wow this is amazing\"`\n",
    "  - **Negative Sentiment**: `\"This is bad\"`\n",
    "- These checks offer qualitative insights into whether the models capture basic sentiment signals and respond logically to inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Select the Best Performing Model**\n",
    "- The model achieving the **highest ROC-AUC** on the validation set and passing the manual inference check will be selected as the **final candidate model**.\n",
    "- This process constitutes **hypothesis class selection**, where validation data is used to evaluate which model architecture generalizes best.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Retrain the Final Model on Full Training + Validation Data**\n",
    "- Once the best model architecture is selected, we **retrain the model from scratch** on a combined dataset consisting of both the original **training** and **validation** sets.\n",
    "- This step maximizes the data available for learning while **preserving the independence of the test set**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Final Evaluation on Test Set**\n",
    "- Evaluate the retrained final model on the **held-out test set**, using:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1 Score\n",
    "  - ROC-AUC\n",
    "  - Confusion Matrix\n",
    "- This test set has **never been used in training or validation**, making it a **clean estimate** of real-world generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Rationale: Bias Mitigation Through Proper Data Segmentation\n",
    "\n",
    "- **Bias is mitigated** by using the **validation set for model/hypothesis selection**, and **excluding the test set** from any decision-making or parameter tuning.\n",
    "- **Data leakage is avoided** because the final test set remains untouched until the very end, and no information from it influences model architecture or hyperparameters.\n",
    "- This separation ensures the **final test performance is an honest estimate** of how the selected model would perform in production or on unseen tweet sentiment data.\n",
    "\n",
    "---\n",
    "\n",
    "*By following this plan, we preserve the scientific integrity of model evaluation, reduce selection bias, and establish a fair, reproducible framework for comparing and deploying tweet sentiment classifiers.*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
