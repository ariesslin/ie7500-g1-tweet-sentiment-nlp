{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6f7be3",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ariesslin/ie7500-g1-tweet-sentiment-nlp/blob/main/scripts/3.%20Model_Development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80",
   "metadata": {
    "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80"
   },
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3. Model Development</strong></h2>\n",
    "  <p style=\"color:#333333;\">Model Selection and Preliminary Performance Testing</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bOElrufQWr4t",
   "metadata": {
    "id": "bOElrufQWr4t"
   },
   "source": [
    "## Model Development Overview: Multi-Architecture Sentiment Classification\n",
    "\n",
    "This notebook presents a structured comparison of three complementary approaches to tweet sentiment classification: **TF-IDF + Logistic Regression**, **Bidirectional LSTM**, and **DistilBERT**. Each model represents a different point on the spectrum from traditional machine learning to state-of-the-art deep learning.\n",
    "\n",
    "### Three-Model Comparison Strategy\n",
    "\n",
    "Our multi-model approach systematically evaluates:\n",
    "\n",
    "1. **Classical Baseline**: TF-IDF + Logistic Regression for interpretable, fast classification\n",
    "2. **Deep Learning**: Bidirectional LSTM for context-aware sequential modeling  \n",
    "3. **Transformer**: DistilBERT for sophisticated contextual understanding\n",
    "\n",
    "### Evaluation Methodology\n",
    "\n",
    "All models are evaluated using consistent metrics including accuracy, precision, recall, F1-score, and computational efficiency to enable fair comparison and informed model selection.\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "This notebook provides high-level overviews of each approach, with detailed implementations available in separate focused notebooks:\n",
    "\n",
    "- **Section 3.1**: Baseline model overview → Also refer to **[3a-Logistic-Regression.ipynb](./3a-Logistic-Regression.ipynb)**\n",
    "- **Section 3.2**: LSTM model overview → Also refer to **[3b-LSTM.ipynb](./3b-LSTM.ipynb)**  \n",
    "- **Section 3.3**: Transformer model overview → Also refer to **[3c-BERT.ipynb](./3c-BERT.ipynb)**\n",
    "- **Section 3.4**: Comparative analysis and results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6",
   "metadata": {
    "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82172ce7-5869-4448-8c89-3e9450c1e426",
   "metadata": {
    "id": "82172ce7-5869-4448-8c89-3e9450c1e426"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.1 Baseline Model – TF-IDF + Logistic Regression</strong></h2>\n",
    "  <p style=\"color:#333333;\">Overview of TF-IDF vectorization + logistic regression baseline model.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa0c91-f0d1-44a5-82c5-789b0bd1e5e7",
   "metadata": {
    "id": "07efb030-35ad-4e70-9186-b72e6bd676ec"
   },
   "source": [
    "## Baseline Model Overview: TF-IDF + Logistic Regression\n",
    "\n",
    "The **TF-IDF + Logistic Regression** model serves as our baseline for tweet sentiment classification. This approach combines term frequency-inverse document frequency vectorization with a linear classifier to provide a strong, interpretable foundation for comparison.\n",
    "\n",
    "### Key Features of the Baseline Model:\n",
    "\n",
    "**TF-IDF Vectorization:**\n",
    "- Converts raw tweets into numerical feature vectors\n",
    "- Measures word importance relative to the entire corpus\n",
    "- Handles sparse, high-dimensional text data effectively\n",
    "- Applies L2 normalization to prevent length bias\n",
    "\n",
    "**Logistic Regression Classifier:**\n",
    "- Linear model that's fast to train and highly interpretable  \n",
    "- Provides probability estimates for sentiment predictions\n",
    "- Works well with TF-IDF's sparse feature representation\n",
    "- Enables coefficient analysis to understand word influences\n",
    "\n",
    "**Hyperparameter Optimization:**\n",
    "- Grid search across TF-IDF parameters (max_features, ngram_range, min_df, max_df)\n",
    "- Logistic regression tuning (regularization strength, class weighting)\n",
    "- 3-fold cross-validation for robust parameter selection\n",
    "- Weighted F1-score optimization for balanced performance\n",
    "\n",
    "### Performance Summary:\n",
    "- **Validation Accuracy**: ~78.16%\n",
    "- **Precision**: ~77.00%\n",
    "- **Recall**: ~80.32%\n",
    "- **F1 Score**: ~78.62%\n",
    "- **Strengths**: Fast, interpretable, solid baseline performance\n",
    "- **Limitations**: Context-insensitive, struggles with negation and nuance\n",
    "\n",
    "This baseline establishes the performance threshold that our deep learning models (LSTM and BERT) should surpass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2c984-64e4-49a4-afab-160dfbc2a57a",
   "metadata": {
    "id": "61b2c984-64e4-49a4-afab-160dfbc2a57a"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.2 Deep Learning Model – Bidirectional LSTM</strong></h2>\n",
    "  <p style=\"color:#333333;\">Bidirectional LSTM with Word2Vec embeddings for sequence-aware sentiment analysis.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d603b-9b00-41df-940e-30e8ad80c8b6",
   "metadata": {
    "id": "to08L5gzucYG"
   },
   "source": [
    "## Deep Learning Sentiment Classifier: Bidirectional LSTM with Word2Vec Embeddings\n",
    "\n",
    "The **Bidirectional LSTM + Word2Vec** model represents our deep learning approach for tweet sentiment classification, designed to capture the sequential nature of language and semantic relationships between words.\n",
    "\n",
    "### Key Features of the LSTM Model:\n",
    "\n",
    "**Word2Vec Embeddings:**\n",
    "- Dense word vectors that capture semantic relationships between words\n",
    "- Trained specifically on our tweet corpus for domain-specific representations  \n",
    "- 100-dimensional embeddings with 100% vocabulary coverage\n",
    "- Words with similar meanings positioned close together in embedding space\n",
    "\n",
    "**Bidirectional LSTM Architecture:**\n",
    "- Processes text in both forward and backward directions for complete context understanding\n",
    "- Two-layer bidirectional LSTM with 128 and 64 hidden units respectively\n",
    "- Dropout layers (0.5 and 0.3) to prevent overfitting during training\n",
    "- Captures long-term dependencies and sequential patterns in tweets\n",
    "\n",
    "**Advanced Features:**\n",
    "- Handles negation, word order, and contextual sentiment better than baseline\n",
    "- Early stopping with patience=3 to prevent overfitting\n",
    "- Nadam optimizer for improved convergence on noisy tweet data\n",
    "- Sequence padding to handle variable tweet lengths efficiently\n",
    "\n",
    "### Performance Summary:\n",
    "- **Validation Accuracy**: ~80.25%\n",
    "- **Precision**: ~79.79%\n",
    "- **Recall**: ~81.02%\n",
    "- **F1 Score**: ~80.40%\n",
    "- **Strengths**: Context-aware, handles sequence patterns, semantic understanding\n",
    "- **Limitations**: Computationally expensive, requires more training time\n",
    "\n",
    "This LSTM model bridges the gap between simple linear models and sophisticated transformers, providing strong performance with interpretable sequential modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b2d45-8f4b-40cb-8728-2a50a2b8f18b",
   "metadata": {
    "id": "164b2d45-8f4b-40cb-8728-2a50a2b8f18b"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdf2dd-b6d2-4007-923c-f7825eba8057",
   "metadata": {
    "id": "aabdf2dd-b6d2-4007-923c-f7825eba8057"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.3 Transformer Model – DistilBERT</strong></h2>\n",
    "  <p style=\"color:#333333;\">Fine-tuning DistilBERT for state-of-the-art contextual sentiment analysis.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b3fc6-5245-4190-b687-99612f572e5c",
   "metadata": {
    "id": "f6c94bb9-609c-486b-8f8b-65b4639e6aa1"
   },
   "source": [
    "## Transformer: DistilBERT Fine-tuning\n",
    "\n",
    "The **DistilBERT** model represents our state-of-the-art approach for tweet sentiment classification, leveraging pre-trained transformer architecture for deep contextual understanding.\n",
    "\n",
    "### Key Features of the DistilBERT Model:\n",
    "\n",
    "**Pre-trained Transformer Architecture:**\n",
    "- Distilled version of BERT with 97% of BERT's performance using 60% fewer parameters\n",
    "- Pre-trained on 16GB of text data, providing rich contextual representations\n",
    "- Bidirectional attention mechanism for complete sentence understanding\n",
    "- Fine-tuned specifically for binary sentiment classification\n",
    "\n",
    "**Advanced NLP Capabilities:**\n",
    "- Handles complex linguistic patterns like sarcasm, negation, and context-dependent sentiment\n",
    "- Understands word relationships across entire tweet sequences simultaneously\n",
    "- Processes subword tokens for better handling of informal social media language\n",
    "- Maximum sequence length of 140 tokens optimized for tweet analysis\n",
    "\n",
    "**Training Configuration:**\n",
    "- 3 epochs with learning rate of 2e-5 for effective fine-tuning\n",
    "- Batch size of 32 for efficient GPU utilization\n",
    "- Early stopping and weight decay for regularization\n",
    "- Optimized sequence length of 96 tokens based on EDA findings\n",
    "\n",
    "### Performance Summary:\n",
    "- **Validation Accuracy**: ~81.18%\n",
    "- **Precision**: ~81.52%\n",
    "- **Recall**: ~80.63%\n",
    "- **F1 Score**: ~81.07%\n",
    "- **Strengths**: Superior contextual understanding, handles complex linguistic patterns\n",
    "- **Limitations**: Computationally expensive, requires significant GPU resources\n",
    "\n",
    "This transformer model achieves the highest overall accuracy and F1 score, demonstrating excellent contextual understanding for robust sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3b1cc-bf9a-4b07-9111-0bfadff815c7",
   "metadata": {
    "id": "21827938-c09d-4fc9-ab90-118c407f2b75"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4741c-4ee7-4cdc-896c-1cde34dd2576",
   "metadata": {
    "id": "a8a4741c-4ee7-4cdc-896c-1cde34dd2576"
   },
   "source": [
    "\n",
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>3.4 Model Comparison and Results Analysis </strong></h2>\n",
    "  <p style=\"color:#333333;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FOTd56MrOLlh",
   "metadata": {
    "id": "FOTd56MrOLlh"
   },
   "source": [
    "## Comprehensive Model Comparison and Performance Analysis\n",
    "\n",
    "This section presents a detailed comparative evaluation of three complementary approaches to tweet sentiment classification: **TF-IDF + Logistic Regression**, **Bidirectional LSTM**, and **DistilBERT**. The analysis encompasses performance metrics, computational efficiency, and practical deployment considerations based on results from the detailed implementation notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Logistic Regression — Validation Results\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy**: 78.16%\n",
    "- **Precision**: 77.00%\n",
    "- **Recall**: 80.32%\n",
    "- **F1 Score**: 78.62%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|                   | **Predicted Negative** | **Predicted Positive** |\n",
    "|-------------------|------------------------|-------------------------|\n",
    "| **Actual Negative** | 54,639                 | 26,013                  |\n",
    "| **Actual Positive** | 15,678                 | 100,000+                |\n",
    "\n",
    "**Sentiment Analysis Insights:**\n",
    "- Logistic Regression shows solid baseline performance and favors **recall**, which means it's highly sensitive to detecting **positive sentiments** in tweets.\n",
    "- However, the high number of **false positives (26,013)** suggests it struggles to distinguish **genuinely negative tweets**, often misclassifying them as positive.\n",
    "- This behavior may be due to:\n",
    "  - Over-simplification of tweet content and lack of contextual understanding.\n",
    "  - Tweets containing mixed signals (e.g., sarcasm or slang) being interpreted incorrectly.\n",
    "\n",
    "- **Use Case Suitability**:\n",
    "  - Adequate for large-scale monitoring where missing positive sentiment is riskier than mistakenly flagging negative ones (e.g., brand loyalty tracking).\n",
    "  - Not ideal where negative sentiment needs precise monitoring (e.g., social crisis detection).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. LSTM — Validation and Training Results\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy**: 80.25%\n",
    "- **Precision**: 79.79%\n",
    "- **Recall**: 81.02%\n",
    "- **F1 Score**: 80.40%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|                   | **Predicted Negative** | **Predicted Positive** |\n",
    "|-------------------|------------------------|-------------------------|\n",
    "| **Actual Negative** | 95,344                 | 24,616                  |\n",
    "| **Actual Positive** | 22,770                 | 97,187                  |\n",
    "\n",
    "**Training Summary:**\n",
    "- Validation accuracy and loss stabilized around epoch 6, suggesting that’s the best point to stop training.\n",
    "- Model learned rapidly early on, but overfitting started appearing after epoch 6.\n",
    "\n",
    "**Sentiment Analysis Insights:**\n",
    "- The LSTM model is especially well-suited for sentiment classification on tweets due to its ability to model **sequence dependencies**.\n",
    "- Tweets often contain **non-standard grammar**, **emojis**, and **elongations** (e.g., \"soooo goood\"), which LSTM handles more effectively than a linear model.\n",
    "- Strong performance in **both recall and precision** implies it can:\n",
    "  - Accurately detect **positive** sentiments.\n",
    "  - Avoid incorrectly labeling **negative tweets** as positive.\n",
    "\n",
    "- **Use Case Suitability**:\n",
    "  - Ideal for real-time sentiment dashboards and public opinion tracking tools.\n",
    "  - Offers a strong balance between catching enthusiastic sentiment and minimizing false optimism.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. BERT — Validation and Training Results\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Accuracy**: 81.18%\n",
    "- **Precision**: 81.52%\n",
    "- **Recall**: 80.63%\n",
    "- **F1 Score**: 81.07%\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "|                   | **Predicted Negative** | **Predicted Positive** |\n",
    "|-------------------|------------------------|-------------------------|\n",
    "| **Actual Negative** | ~60,000                | 20,991                  |\n",
    "| **Actual Positive** | 16,150                 | 100,000+                |\n",
    "\n",
    "**Training Summary:**\n",
    "- Training completed in ~31 minutes with early stopping after detecting optimal performance.\n",
    "- Mixed precision training and optimized sequence length (96 tokens) enabled efficient fine-tuning.\n",
    "\n",
    "**Sentiment Analysis Insights:**\n",
    "- DistilBERT achieves the best **overall accuracy** and **F1 score**, demonstrating balanced performance across all metrics.\n",
    "- The transformer's attention mechanism excels at capturing contextual nuances in tweets with complex language patterns.\n",
    "- Slightly lower recall compared to LSTM suggests DistilBERT is more conservative in positive predictions, leading to higher precision in sentiment detection.\n",
    "\n",
    "- **Use Case Suitability**:\n",
    "  - Excellent for **automated sentiment scoring**, content moderation, or **flagging key influencer reactions**.\n",
    "  - Especially beneficial when **false positives** (mistakenly labeling negativity as positivity) are more damaging (e.g., in reputation management or crisis alerts).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Comprehensive Model Comparison and Performance Analysis\n",
    "\n",
    "| **Metric**              | **TF-IDF + LogReg** | **LSTM**           | **DistilBERT**     |\n",
    "|-------------------------|---------------------|--------------------|--------------------| \n",
    "| **Accuracy**            | 78.16%              | 80.25%             | **81.18%**         |\n",
    "| **Precision**           | 77.00%              | 79.79%             | **81.52%**         |\n",
    "| **Recall**              | 80.32%              | **81.02%**         | 80.63%             |\n",
    "| **F1 Score**            | 78.62%              | 80.40%             | **81.07%**         |\n",
    "| **False Positives**     | 26,013              | 24,616             | **20,991**         |\n",
    "| **False Negatives**     | 15,678              | **22,770**         | 16,150             |\n",
    "| **Training Time**       | ~5 minutes          | ~11 minutes (early stop)       | ~30 minutes (early stop)        |\n",
    "| **Inference Speed**     | **Very Fast**       | Fast               | Moderate           |\n",
    "| **Interpretability**    | **High**            | Low                | Low                |\n",
    "| **Resource Needs**      | **Low**             | Moderate           | High               |\n",
    "| **Context Understanding**| Limited            | Good               | **Excellent**      |\n",
    "\n",
    "**Overall Insights:**\n",
    "- **Logistic Regression** remains a decent choice for high-volume, low-compute environments, where identifying most of the **positive sentiments** is more critical than precision.\n",
    "- **LSTM** is the most **balanced** model, particularly effective at picking up **positive sentiment** while minimizing false detections. Its sequential modeling helps handle emotive expressions common in tweets.\n",
    "- **BERT** offers the best **precision**, indicating it’s the most confident and contextually aware when assigning **positive sentiment**, but it may miss tweets that use creative or ambiguous language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ea7f3-de45-49a6-83cb-e5014a4ead19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Decision Framework for Model Selection\n",
    "\n",
    "#### Choose **TF-IDF + Logistic Regression** When:\n",
    "- **Speed is Critical**: Sub-second response times required\n",
    "- **Resource Constraints**: CPU-only environments or extreme scale requirements  \n",
    "- **Interpretability Needed**: Must understand individual feature contributions\n",
    "- **Rapid Prototyping**: Quick experimentation and baseline establishment\n",
    "\n",
    "#### Choose **Bidirectional LSTM** When:\n",
    "- **Balanced Performance**: Need good precision-recall balance\n",
    "- **Sequential Understanding**: Context awareness without full transformer overhead\n",
    "- **Mid-Range Hardware**: Limited to consumer-grade GPUs\n",
    "- **Real-time Applications**: Sentiment dashboards and monitoring systems\n",
    "\n",
    "#### Choose **DistilBERT** When:\n",
    "- **Accuracy is Critical**: Small improvements justify computational cost\n",
    "- **Complex Language Patterns**: Must handle sarcasm, negation, context-dependent sentiment\n",
    "- **Sufficient Resources**: GPU infrastructure and computational budget available\n",
    "- **Production Quality**: Building customer-facing or high-stakes applications\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights and Recommendations\n",
    "\n",
    "**Performance Progression:**\n",
    "The models demonstrate clear advancement from traditional ML to deep learning, with accuracy improvements of ~2.1% (baseline to LSTM) and ~3.0% (baseline to DistilBERT).\n",
    "\n",
    "**Computational Trade-offs:**\n",
    "- DistilBERT achieves highest precision but requires **2.7x longer training** than LSTM (30 vs 11 minutes)\n",
    "- Early stopping significantly reduced training times for both deep learning models while maintaining performance\n",
    "- LSTM provides efficient training with good performance gains over the baseline\n",
    "\n",
    "**Error Analysis:**\n",
    "- **False Positives**: DistilBERT (20,991) < LSTM (24,616) < LogReg (26,013)\n",
    "- **False Negatives**: LogReg (15,678) < DistilBERT (16,150) < LSTM (22,770)\n",
    "- **DistilBERT** shows the best overall error distribution with lowest false positives\n",
    "\n",
    "**Deployment Recommendations:**\n",
    "- **High-Throughput Systems**: TF-IDF + Logistic Regression\n",
    "- **Balanced Production Applications**: Bidirectional LSTM  \n",
    "- **Accuracy-Critical Applications**: DistilBERT with optimization techniques\n",
    "\n",
    "This comprehensive comparison enables informed model selection based on specific application requirements, computational constraints, and performance priorities.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
