{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff2a8f2",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ariesslin/ie7500-g1-tweet-sentiment-nlp/blob/main/scripts/2.%20Data%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80",
   "metadata": {
    "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80"
   },
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>2. Data Preprocessing</strong></h2>\n",
    "  <p style=\"color:#333333;\">Preparing data for model training and testing</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6",
   "metadata": {
    "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6"
   },
   "source": [
    "Raw tweet data is often noisy, inconsistent, and full of informal elements like emojis, mentions, hashtags, abbreviations, and inconsistent casing. Cleaning and standardizing this data is a critical step before any meaningful modeling can be done. Effective preprocessing not only improves model performance but also ensures that the insights learned from the data are robust and generalizable.\n",
    "\n",
    "In this notebook, we apply a series of preprocessing steps tailored specifically to Twitter data, with the goal of simplifying the input while preserving sentiment-relevant patterns. The following steps will be applied to the Sentiment140 dataset:\n",
    "\n",
    "- Convert all uppercase words to lowercase for consistency\n",
    "- Replace all Twitter mentions (e.g., `@username`) with the token `\"MENTION\"` to retain structure without leaking identity-specific information\n",
    "- Download necessary resources using `nltk.download()` (e.g., `stopwords`, `punkt`)\n",
    "- Define a custom stopword list by combining standard NLTK stopwords with common punctuation marks (e.g., `,`, `.`, `\"`, `'`)\n",
    "- Remove extra white space and strip leading/trailing spaces\n",
    "\n",
    "These steps help remove noise while preserving semantic structure, which is especially important for models like LSTM and BERT that are sensitive to input tokenization.\n",
    "\n",
    "Finally, we will **split the dataset into training and testing subsets** to evaluate the models' generalization performance on unseen data.\n",
    "\n",
    "This preprocessing pipeline prepares our dataset for vectorization and embedding, enabling classical models like Logistic Regression as well as deep learning architectures to learn from tweet text effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad962f3b-7abc-4d06-a83d-22f7bc0bdcfc",
   "metadata": {
    "id": "ad962f3b-7abc-4d06-a83d-22f7bc0bdcfc"
   },
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>1. Data Loading & Overview</strong></h2>\n",
    "  <p style=\"color:#333333;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ugwVdiT38xCV",
   "metadata": {
    "id": "ugwVdiT38xCV"
   },
   "outputs": [],
   "source": [
    "# Importing all libraries here\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6f7e89-bc3b-4c97-af11-b444896771b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions modularized from our EDA notebook\n",
    "sys.path.append(\"../utils\")\n",
    "from helper import load_sentiment140, preprocess_raw_text_in_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "W3Dq_M6fW_tQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3Dq_M6fW_tQ",
    "outputId": "c32b8042-674e-47f9-fa41-d143c2b290fc"
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Sentiment140 dataframe\n",
    "df = load_sentiment140()\n",
    "\n",
    "# Step 2: Preprocess raw text (decode, unescape HTML characters, remove empty rows, add length)\n",
    "df = preprocess_raw_text_in_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "_rIqGmOLXjd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rIqGmOLXjd5",
    "outputId": "fae10e09-f646-4edd-d5a3-aa9f12a7d1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets longer than 140 characters: 0\n"
     ]
    }
   ],
   "source": [
    "# Count how many tweets are longer than 140 characters\n",
    "num_long_tweets = df[df['text_length'] > 140].shape[0]\n",
    "print(f\"Number of tweets longer than 140 characters: {num_long_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "EmzyPmhzYBpT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "EmzyPmhzYBpT",
    "outputId": "f8d9e455-943f-487a-c4a6-c63335666f82"
   },
   "outputs": [],
   "source": [
    "# Disable truncation in display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Sort by tweet length and show the top 10 longest tweets\n",
    "df.sort_values(by='text_length', ascending=False)[['text', 'text_length']].head(10)\n",
    "\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9453c7-a7af-4299-8988-1089f8f42fb9",
   "metadata": {},
   "source": [
    "#### Data Loading and First Step Preprocessing Summary\n",
    "\n",
    "From the above result, we confirm that the Sentiment140 dataset has been **successfully loaded and preprocessed** using the same steps developed during the EDA phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c6dda3-8402-492c-acf1-956a2efcd7e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10c6dda3-8402-492c-acf1-956a2efcd7e6",
    "outputId": "d718b375-3ce6-4db7-e64b-6ceb3d2496e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ariesslin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ariesslin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords and punkt_tab\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82MUpULOPVk8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82MUpULOPVk8",
    "outputId": "e2587666-7c68-4498-9a9f-c32e1e574720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iEDvbM5VPU-m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "iEDvbM5VPU-m",
    "outputId": "2b081dd8-b532-4784-c8a2-39330623683a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5m/q16b86r92zgbmjpxpdx4ghfw0000gn/T/ipykernel_19737/2929434613.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['text'] = tweets['text'].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  @switchfoot http://twitpic.com/2y1zl - awww, t...       0\n",
       "1  is upset that he can't update his facebook by ...       0\n",
       "2  @kenichan i dived many times for the ball. man...       0\n",
       "3    my whole body feels itchy and like its on fire        0\n",
       "4  @nationwideclass no, it's not behaving at all....       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1, Convert all cases to lower\n",
    "\n",
    "tweets = df[['text', 'target']]\n",
    "\n",
    "tweets['text'] = tweets['text'].str.lower()\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac7135d-4ecd-4c49-9538-eea9d023e4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9ac7135d-4ecd-4c49-9538-eea9d023e4bb",
    "outputId": "2c4d70ea-88d0-4c7b-aa1e-f697fe202b40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention http://twitpic.com/2y1zl - awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention i dived many times for the ball. m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention no, it's not behaving at all. i'm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  usermention http://twitpic.com/2y1zl - awww, t...       0\n",
       "1  is upset that he can't update his facebook by ...       0\n",
       "2  usermention i dived many times for the ball. m...       0\n",
       "3    my whole body feels itchy and like its on fire        0\n",
       "4  usermention no, it's not behaving at all. i'm ...       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2, convert all mentions and the mentioned account names to MENTION\n",
    "\n",
    "# Make a copy to avoid modifying the original DataFrame\n",
    "tweets = df[['text', 'target']].copy()\n",
    "\n",
    "# Convert to lowercase\n",
    "tweets['text'] = tweets['text'].str.lower()\n",
    "\n",
    "# Replace all @mentions with \"usermention\"\n",
    "tweets['text'] = tweets['text'].str.replace(r'@\\w+', 'usermention', regex=True)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "_MFt2E_spfxG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MFt2E_spfxG",
    "outputId": "38140ade-3256-4e25-e6f2-098415cf54d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'].apply(lambda x: pd.isna(x) or str(x).strip() == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ex-V6ZTsSXBn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "ex-V6ZTsSXBn",
    "outputId": "175e1af3-dfa6-4709-e186-1bbb6a915577"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ariesslin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention thats bummer shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention dived many time ball managed save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  usermention thats bummer shoulda got david car...      0\n",
       "1  upset cant update facebook texting might cry r...      0\n",
       "2  usermention dived many time ball managed save ...      0\n",
       "3                    whole body feel itchy like fire      0\n",
       "4               usermention behaving im mad cant see      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step #3, we tokenize the words and remove stop words\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Setup\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tweets_array = tweets.to_numpy(copy=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add extra symbols to stop words\n",
    "stop_words.update(string.punctuation)\n",
    "stop_words.update([\"''\", \"'\", '``', '’', '“', '”','–', '—', '…', '..', '.', ',', ':', ';', '?', '!', '(', ')', '[', ']', '{', '}', '/', '|'])\n",
    "\n",
    "# Stemmers / lemmatizers\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess each tweet\n",
    "for i in range(len(tweets_array)):\n",
    "    text = tweets_array[i][0]  # Access the tweet text\n",
    "\n",
    "    # Remove numbers and whitespace\n",
    "    text = ''.join((z for z in text if not z.isdigit()))\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "    # Tokenize using TreebankWordTokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove stopwords but keep usermention\n",
    "    tokens = [word for word in tokens if word not in stop_words or word == 'usermention']\n",
    "\n",
    "    # Lemmatize (skip MENTION)\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) if word != 'usermention' else word for word in tokens]\n",
    "\n",
    "    # Rejoin\n",
    "    tweets_array[i][0] = ' '.join(tokens)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "preprocessed_tweets = pd.DataFrame(tweets_array, columns=['text', 'target'])\n",
    "preprocessed_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lfGayIZ4pjRP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfGayIZ4pjRP",
    "outputId": "961c9b41-1d05-4cae-f481-d5d90b09029e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets['text'].apply(lambda x: pd.isna(x) or str(x).strip() == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U0kTBjcWp3e-",
   "metadata": {
    "id": "U0kTBjcWp3e-"
   },
   "source": [
    "**we have around 500 \"new\" null text cells introduced after removing stop words. they will be deleted later as a first step before sampling training, validation and testing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z2bj2-WBiF9B",
   "metadata": {
    "id": "Z2bj2-WBiF9B"
   },
   "source": [
    "#### Here we view word cloud of tweets for negative and positive marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AT-nJLMniMXw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "AT-nJLMniMXw",
    "outputId": "dec4c460-233a-460b-e20e-b34917aba2ec"
   },
   "outputs": [],
   "source": [
    "# Negative Tweets word cloud\n",
    "\n",
    "# Filter only negative tweets (target == 0)\n",
    "negative_tweets = preprocessed_tweets[preprocessed_tweets['target'] == 0]['text']\n",
    "\n",
    "# Combine all negative tweet text into a single string\n",
    "text_blob = ' '.join(negative_tweets.astype(str))\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Reds',\n",
    "    max_words=200\n",
    ").generate(text_blob)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Negative Tweets\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9B81T2iMBf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "0b9B81T2iMBf",
    "outputId": "8007c94f-b3ea-4e1e-c72d-7c64b28e04bc"
   },
   "outputs": [],
   "source": [
    "# Positive Tweets word cloud\n",
    "\n",
    "# Filter only positive tweets (target == 4)\n",
    "positive_tweets = preprocessed_tweets[preprocessed_tweets['target'] == 4]['text']\n",
    "\n",
    "# Combine all negative tweet text into a single string\n",
    "text_blob = ' '.join(positive_tweets.astype(str))\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Greens',\n",
    "    max_words=200\n",
    ").generate(text_blob)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Positive Tweets\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5tQEU1XeSWzW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5tQEU1XeSWzW",
    "outputId": "46b23813-7aab-47f0-d18e-8a682c22a657"
   },
   "outputs": [],
   "source": [
    "# We now save the processed tweets to be used later in model development\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Save file inside Colab\n",
    "processed_data_dir = \"/content\"  # Best practice for Colab\n",
    "output_processed_file_path = os.path.join(processed_data_dir, 'preprocessed_tweets.csv')\n",
    "preprocessed_tweets.to_csv(output_processed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed tweets saved to: {output_processed_file_path}\")\n",
    "\n",
    "# Step 2: Download Mac Downloads folder manually\n",
    "files.download(output_processed_file_path)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Save processed file in github as it is less than 100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f8414-d432-4984-92da-02f7dfd7f62e",
   "metadata": {
    "id": "863f8414-d432-4984-92da-02f7dfd7f62e",
    "outputId": "e25d4f94-3ce8-4343-cd98-fc03241f6678"
   },
   "outputs": [],
   "source": [
    "processed_data_dir = \"../processed_data\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "output_processed_file_path = os.path.join(processed_data_dir, \"preprocessed_tweets.csv\")\n",
    "preprocessed_tweets.to_csv(output_processed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed tweets saved to: {output_processed_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NfmIEPPkkeyd",
   "metadata": {
    "id": "NfmIEPPkkeyd"
   },
   "source": [
    "#### Next we take first 70% of each negative and positive tweets for training, 15% for validation, and 15% for testing.\n",
    "Sampling sould be done in a way that respects tweets length distribution.\n",
    "\n",
    "This way we make sure all models are trained and validated using the same dataset and we have a good ground for fair comparison.\n",
    "\n",
    "This step is valid because we showed in EDA section that negative and positive tewwts are equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6a331-ac35-4764-b7a4-47f698522c51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "f8f6a331-ac35-4764-b7a4-47f698522c51",
    "outputId": "36b6e488-f09c-47a3-e96b-af26adcd4147"
   },
   "outputs": [],
   "source": [
    "# Here, we begin by loading the processed dataset\n",
    "\"\"\"\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "file_id = \"1LiS2ltl2XfITCamIrERkaa69OKKKoR_r\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "output = \"../data/preprocessed_tweets.csv\"\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8da74-abb8-45b1-a637-e57386b02a33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "2ba8da74-abb8-45b1-a637-e57386b02a33",
    "outputId": "1958ebd1-63d1-4a9b-d9df-101de692ef62"
   },
   "outputs": [],
   "source": [
    "preprocessed_path = \"../processed_data/preprocessed_tweets.csv\"\n",
    "Tweets = pd.read_csv(preprocessed_path)\n",
    "\n",
    "# Read the preprocessed tweet dataset and assign column names\n",
    "# Tweets = pd.read_csv(\"../data/preprocessed_tweets.csv\", header=None, names=[\"text\", \"target\"])\n",
    "\n",
    "# Drop any rows where 'text' or 'target' is null\n",
    "Tweets = Tweets.dropna(subset=[\"text\", \"target\"])\n",
    "\n",
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fXfKl95mkCwK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXfKl95mkCwK",
    "outputId": "44d72cb5-5093-4b82-b0d9-f279387cdb11"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels from the bar chart in our EDA\n",
    "bin_edges = [0, 20, 40, 60, 80, 100, 120, 140]\n",
    "bin_labels = ['0–20', '21–40', '41–60', '61–80', '81–100', '101–120', '121–140']\n",
    "\n",
    "# Step 1: Assign length buckets safely\n",
    "def assign_length_buckets(df):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['text'].fillna('').astype(str)  # Ensure no NaNs\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['length_bucket'] = pd.cut(\n",
    "        df['text_length'],\n",
    "        bins=bin_edges,\n",
    "        labels=bin_labels,\n",
    "        right=True,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    df = df.dropna(subset=['length_bucket'])  # Drop any that didn't fall in bin\n",
    "    return df\n",
    "\n",
    "# Step 2: Stratified split function\n",
    "def stratified_split_by_length_bucket(df_class, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    df_class = assign_length_buckets(df_class)\n",
    "\n",
    "    # Split into train and temp\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df_class,\n",
    "        test_size=1 - train_ratio,\n",
    "        stratify=df_class['length_bucket'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Split temp into validation and test\n",
    "    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=1 - val_ratio_adjusted,\n",
    "        stratify=temp_df['length_bucket'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return train_df.drop(columns='length_bucket'), val_df.drop(columns='length_bucket'), test_df.drop(columns='length_bucket')\n",
    "\n",
    "# Step 3: Filter by sentiment\n",
    "df_neg = Tweets[Tweets['target'] == 0]\n",
    "df_pos = Tweets[Tweets['target'] == 4]\n",
    "\n",
    "# Step 4: Apply stratified splitting\n",
    "neg_train, neg_val, neg_test = stratified_split_by_length_bucket(df_neg)\n",
    "pos_train, pos_val, pos_test = stratified_split_by_length_bucket(df_pos)\n",
    "\n",
    "# Step 5: Combine and shuffle\n",
    "train_df = pd.concat([neg_train, pos_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = pd.concat([neg_val, pos_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat([neg_test, pos_test]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Extract input and labels\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['target']\n",
    "X_val = val_df['text']\n",
    "y_val = val_df['target']\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['target']\n",
    "\n",
    "# Final check\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"X_train shape:\", X_train.shape, \"| y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape, \"| y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape, \"| y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xR0QU6tft5l2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "xR0QU6tft5l2",
    "outputId": "a943eb95-be38-4b20-e057-bbb4e19a64d8"
   },
   "outputs": [],
   "source": [
    "# here we validate the distributions are identical and intact\n",
    "\n",
    "# Combine text + label for plotting, and calculate text length\n",
    "train_plot_df = pd.concat([X_train, y_train], axis=1).copy()\n",
    "val_plot_df = pd.concat([X_val, y_val], axis=1).copy()\n",
    "test_plot_df = pd.concat([X_test, y_test], axis=1).copy()\n",
    "\n",
    "# Ensure text_length column is present\n",
    "train_plot_df['text_length'] = train_plot_df['text'].str.len()\n",
    "val_plot_df['text_length'] = val_plot_df['text'].str.len()\n",
    "test_plot_df['text_length'] = test_plot_df['text'].str.len()\n",
    "\n",
    "# Ensure 'target' is integer (not string) for palette mapping\n",
    "train_plot_df['target'] = train_plot_df['target'].astype(int)\n",
    "val_plot_df['target'] = val_plot_df['target'].astype(int)\n",
    "test_plot_df['target'] = test_plot_df['target'].astype(int)\n",
    "\n",
    "# Define sentiment color palette\n",
    "sentiment_palette = {\n",
    "    0: '#fc8d62',  # Negative\n",
    "    4: '#66c2a5'   # Positive\n",
    "}\n",
    "\n",
    "# Set up subplot layout\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Plot train\n",
    "sns.violinplot(ax=axes[0], data=train_plot_df, x='target', y='text_length', hue='target', palette=sentiment_palette, inner='quartile', legend=False)\n",
    "axes[0].set_title('Train Set')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels(['Negative (0)', 'Positive (4)'])\n",
    "\n",
    "# Plot validation\n",
    "sns.violinplot(ax=axes[1], data=val_plot_df, x='target', y='text_length', hue='target', palette=sentiment_palette, inner='quartile', legend=False)\n",
    "axes[1].set_title('Validation Set')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels(['Negative (0)', 'Positive (4)'])\n",
    "\n",
    "# Plot test\n",
    "sns.violinplot(ax=axes[2], data=test_plot_df, x='target', y='text_length', hue='target', palette=sentiment_palette, inner='quartile', legend=False)\n",
    "axes[2].set_title('Test Set')\n",
    "axes[2].set_xlabel('Sentiment')\n",
    "axes[2].set_xticks([0, 1])\n",
    "axes[2].set_xticklabels(['Negative (0)', 'Positive (4)'])\n",
    "\n",
    "# Shared Y-label and layout\n",
    "fig.supylabel('Tweet Length (characters)', fontsize=12)\n",
    "plt.suptitle(\"Tweet Length Distribution by Sentiment Across Splits\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rzg0gE-8vxA6",
   "metadata": {
    "id": "Rzg0gE-8vxA6"
   },
   "source": [
    "### After we made sure we have good and valid splits, we now move on to modeling part, where we will only use training and validation set for model comparison and selection, ***testing dataset will only be used in model performance evaluation section***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gIWg8bWFMyjE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "gIWg8bWFMyjE",
    "outputId": "30c6a7e4-a548-460f-a7a4-1c707925434a"
   },
   "outputs": [],
   "source": [
    "# Finally, we save splits as CSV to ensure no data leakage takes place as differnt team members split and train the models. this way everyone uses the same training dataset.\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Save splits locally (optional for inspection or reuse)\n",
    "train_df.to_csv(\"train_dataset.csv\", index=False)\n",
    "val_df.to_csv(\"val_dataset.csv\", index=False)\n",
    "test_df.to_csv(\"test_dataset.csv\", index=False)\n",
    "\n",
    "# Step 2: Offer download in Colab (optional)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"train_dataset.csv\")\n",
    "    files.download(\"val_dataset.csv\")\n",
    "    files.download(\"test_dataset.csv\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab. Local files saved.\")\n",
    "\n",
    "# Step 3: Save processed datasets to GitHub repo structure\n",
    "print(\"Saving split datasets and compressed archive in repository...\")\n",
    "\n",
    "processed_data_dir = \"../processed_data\" if os.path.exists(\"../processed_data\") else \"processed_data\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# Paths to save the individual files\n",
    "train_path = os.path.join(processed_data_dir, \"train_dataset.csv\")\n",
    "val_path = os.path.join(processed_data_dir, \"val_dataset.csv\")\n",
    "test_path = os.path.join(processed_data_dir, \"test_dataset.csv\")\n",
    "\n",
    "# Save to processed_data/\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "# Step 4: Create a zip archive of the datasets\n",
    "zip_path = os.path.join(processed_data_dir, \"sentiment140_splits.zip\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(train_path, arcname=\"train_dataset.csv\")\n",
    "    zipf.write(val_path, arcname=\"val_dataset.csv\")\n",
    "    zipf.write(test_path, arcname=\"test_dataset.csv\")\n",
    "\n",
    "print(f\"Zipped dataset saved to: {zip_path}\")\n",
    "print(\"Contents of archive:\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "    print(zipf.namelist())\n",
    "\n",
    "print(\"\\n📦 Ready to commit the archive to your GitHub repository:\")\n",
    "print(f\"cd to repo folder and run:\\n\")\n",
    "print(f\"git add {zip_path}\")\n",
    "print(f'git commit -m \"Add compressed dataset splits for reproducibility\"')\n",
    "print(\"git push\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
