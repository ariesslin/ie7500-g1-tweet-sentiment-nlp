{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff2a8f2",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ariesslin/ie7500-g1-tweet-sentiment-nlp/blob/main/scripts/2.%20Data%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80",
   "metadata": {
    "id": "10d67c03-2ae7-41f6-81d0-56fe632ece80"
   },
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>2. Data Preprocessing</strong></h2>\n",
    "  <p style=\"color:#333333;\">Preparing data for model training and testing</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6",
   "metadata": {
    "id": "5ad9fdb6-b723-469c-a345-f2bbed6055a6"
   },
   "source": [
    "Raw tweet data is often noisy, inconsistent, and full of informal elements like emojis, mentions, hashtags, abbreviations, and inconsistent casing. Cleaning and standardizing this data is a critical step before any meaningful modeling can be done. Effective preprocessing not only improves model performance but also ensures that the insights learned from the data are robust and generalizable.\n",
    "\n",
    "In this notebook, we apply a series of preprocessing steps tailored specifically to Twitter data, with the goal of simplifying the input while preserving sentiment-relevant patterns. The following steps will be applied to the Sentiment140 dataset:\n",
    "\n",
    "- **Lowercasing** – Convert all text to lowercase to ensure consistency and reduce redundant token forms (e.g., \"Happy\" vs. \"happy\").\n",
    "- **Mention, Hashtag & URL Handling** – Replace Twitter mentions (e.g., @username) with a generic token (`usermention`), remove URLs that typically do not contribute to sentiment, and normalize hashtags (e.g., `#happy` → `happy`).\n",
    "- **Number and whitespace cleanup** – After token processing, remove excess spaces and strip leading/trailing whitespace.\n",
    "- **Punctuation & Special Character Removal** – Remove common punctuation marks and special characters to simplify the token vocabulary and reduce noise.\n",
    "- **Tokenization** – Split each tweet into a list of word tokens for structured processing in downstream models.\n",
    "- **Stopword Removal** – Filter out common stopwords (e.g., \"and\", \"is\") that carry little semantic weight, using an extended list that includes punctuation.\n",
    "- **Lemmatization** – Reduce words to their dictionary (base) form to unify variations and decrease vocabulary size (e.g., \"running\" → \"run\").\n",
    "- **Empty Tweet Filtering** – Drop tweets that are empty after preprocessing to ensure that all inputs to the model are meaningful.\n",
    "\n",
    "These steps help remove noise while preserving semantic structure, which is especially important for models like LSTM and BERT that are sensitive to input tokenization.\n",
    "\n",
    "Finally, we will **split the dataset into training, validation and testing subsets** to evaluate the models' generalization performance on unseen data.\n",
    "\n",
    "This preprocessing pipeline prepares our dataset for vectorization and embedding, enabling classical models like Logistic Regression as well as deep learning architectures to learn from tweet text effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad962f3b-7abc-4d06-a83d-22f7bc0bdcfc",
   "metadata": {
    "id": "ad962f3b-7abc-4d06-a83d-22f7bc0bdcfc"
   },
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>1. Data Loading & Preprocessing</strong></h2>\n",
    "  <p style=\"color:#333333;\">Import Sentiment140 dataset and preprocess it with steps we already did during EDA.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ugwVdiT38xCV",
   "metadata": {
    "id": "ugwVdiT38xCV"
   },
   "outputs": [],
   "source": [
    "# Importing all libraries here\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6f7e89-bc3b-4c97-af11-b444896771b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions modularized from our EDA notebook\n",
    "sys.path.append(\"../utils\")\n",
    "from helper import load_sentiment140, preprocess_raw_text_in_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "W3Dq_M6fW_tQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3Dq_M6fW_tQ",
    "outputId": "c32b8042-674e-47f9-fa41-d143c2b290fc"
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Sentiment140 dataframe\n",
    "df = load_sentiment140()\n",
    "\n",
    "# Step 2: Preprocess raw text (decode, unescape HTML characters, remove empty rows, add length)\n",
    "df = preprocess_raw_text_in_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "_rIqGmOLXjd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rIqGmOLXjd5",
    "outputId": "fae10e09-f646-4edd-d5a3-aa9f12a7d1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets longer than 140 characters: 0\n"
     ]
    }
   ],
   "source": [
    "# Count how many tweets are longer than 140 characters\n",
    "num_long_tweets = df[df['text_length'] > 140].shape[0]\n",
    "print(f\"Number of tweets longer than 140 characters: {num_long_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "EmzyPmhzYBpT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "EmzyPmhzYBpT",
    "outputId": "f8d9e455-943f-487a-c4a6-c63335666f82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1462693</th>\n",
       "      <td>i am NOT liking the \"big-curly-hair-with-the-braids\" look :/ im not taking my braids out! i will just simply straighten my hair once more</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402277</th>\n",
       "      <td>Woke up to a storm outside...Florida gets on my nerves sometimes...the state not the lady from Good Times...except the episode when...nvm</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307540</th>\n",
       "      <td>@ohmishka I thought it was such a great idea too, she said she would rather support independent artists than give her $ to big companies.</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805844</th>\n",
       "      <td>I like penis: Ok so backup only was not enough for you pussys, HERE, have some credit cards too  You know, it'.. http://tinyurl.com/d356b3</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805840</th>\n",
       "      <td>Selling DILDO in a box: Ok so backup only was not enough for you pussys, HERE, have some credit cards too  You.. http://tinyurl.com/ca2ecn</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111822</th>\n",
       "      <td>http://twiturm.com/vew6 I made this mix just for fun. I used to live w/ 1 of the BCR (not as a bf) though  in Lake Forest, CA.  Trying ...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975039</th>\n",
       "      <td>@chiropractic ha! you are my hero. weekend stats looking good over here, lots of people dreading their job come monday, searching 4 new 1</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252190</th>\n",
       "      <td>Is finally back from London after being stuck in traffic for about 5 hours! I now have hardly any time to get work done. Long night ahead</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626735</th>\n",
       "      <td>@brennygee Looks like the twitterverse disagrees with you  I just figured my first iPod was white- I thought I'd be kicking it old school.</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307506</th>\n",
       "      <td>yup its going to be @TaqiyyaLuvLa @10marion @officialTila @Tyrese4Real @Willie_Day26 @souljaboytellem and many more tonight,fun...  lol jk</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                               text  \\\n",
       "1462693  i am NOT liking the \"big-curly-hair-with-the-braids\" look :/ im not taking my braids out! i will just simply straighten my hair once more    \n",
       "1402277  Woke up to a storm outside...Florida gets on my nerves sometimes...the state not the lady from Good Times...except the episode when...nvm    \n",
       "1307540  @ohmishka I thought it was such a great idea too, she said she would rather support independent artists than give her $ to big companies.    \n",
       "805844   I like penis: Ok so backup only was not enough for you pussys, HERE, have some credit cards too  You know, it'.. http://tinyurl.com/d356b3   \n",
       "805840   Selling DILDO in a box: Ok so backup only was not enough for you pussys, HERE, have some credit cards too  You.. http://tinyurl.com/ca2ecn   \n",
       "1111822  http://twiturm.com/vew6 I made this mix just for fun. I used to live w/ 1 of the BCR (not as a bf) though  in Lake Forest, CA.  Trying ...   \n",
       "975039   @chiropractic ha! you are my hero. weekend stats looking good over here, lots of people dreading their job come monday, searching 4 new 1    \n",
       "252190   Is finally back from London after being stuck in traffic for about 5 hours! I now have hardly any time to get work done. Long night ahead    \n",
       "626735   @brennygee Looks like the twitterverse disagrees with you  I just figured my first iPod was white- I thought I'd be kicking it old school.   \n",
       "1307506  yup its going to be @TaqiyyaLuvLa @10marion @officialTila @Tyrese4Real @Willie_Day26 @souljaboytellem and many more tonight,fun...  lol jk   \n",
       "\n",
       "         text_length  \n",
       "1462693          138  \n",
       "1402277          138  \n",
       "1307540          138  \n",
       "805844           138  \n",
       "805840           138  \n",
       "1111822          138  \n",
       "975039           138  \n",
       "252190           138  \n",
       "626735           138  \n",
       "1307506          138  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable truncation in display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Sort by tweet length and show the top 10 longest tweets\n",
    "df.sort_values(by='text_length', ascending=False)[['text', 'text_length']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9721f4-3bdf-4aa1-a7f3-b46f8539c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9453c7-a7af-4299-8988-1089f8f42fb9",
   "metadata": {},
   "source": [
    "#### Data Loading and First Step Preprocessing Summary\n",
    "\n",
    "From the above result, we confirm that the Sentiment140 dataset has been **successfully loaded and preprocessed** using the same steps developed during the EDA phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56a4a8-1028-436b-9a18-c572312af185",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f2ff; border-left:8px solid #0059b3; padding:20px; margin:20px 0;\">\n",
    "  <h2 style=\"color:#003366;\"><strong>2. Data Preprocessing</strong></h2>\n",
    "  <p style=\"color:#333333;\">Steps: lowercasing, punctuation & stopword removal, tokenization, stemming, lemmatization, cleaning hashtags, mentions, URLs, numbers.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c6dda3-8402-492c-acf1-956a2efcd7e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10c6dda3-8402-492c-acf1-956a2efcd7e6",
    "outputId": "d718b375-3ce6-4db7-e64b-6ceb3d2496e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ariesslin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ariesslin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ariesslin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords, punkt and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82MUpULOPVk8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82MUpULOPVk8",
    "outputId": "e2587666-7c68-4498-9a9f-c32e1e574720"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    # Only download if the model is not already installed\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb01be4a-b351-4730-8e94-82c3eb7c9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP model (includes word vectors and lemmatizer)\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154bd6e-eb8c-4da0-896b-28a7806c97d7",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "We lowercase all text to reduce redundancy — for example, \"Happy\" and \"happy\" should be treated the same. This standardization helps models treat words as case-insensitive unless case carries specific meaning (which it usually doesn't in tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "iEDvbM5VPU-m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "iEDvbM5VPU-m",
    "outputId": "2b081dd8-b532-4784-c8a2-39330623683a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  @switchfoot http://twitpic.com/2y1zl - awww, t...       0\n",
       "1  is upset that he can't update his facebook by ...       0\n",
       "2  @kenichan i dived many times for the ball. man...       0\n",
       "3    my whole body feels itchy and like its on fire        0\n",
       "4  @nationwideclass no, it's not behaving at all....       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a clean copy of relevant columns\n",
    "tweets = df.loc[:, ['text', 'target']].copy()\n",
    "\n",
    "# Apply lowercase\n",
    "tweets['text'] = tweets['text'].str.lower()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590f203-cfe3-47cc-99fc-cd6ba242c6ab",
   "metadata": {},
   "source": [
    "### Number and Whitespace Cleanup\n",
    "\n",
    "We remove numeric digits from tweets to prevent potential noise from irrelevant numbers (e.g., user IDs, dates, counts). Then we normalize whitespace by converting multiple consecutive spaces into a single space and trimming leading/trailing whitespace. This ensures clean, consistent input formatting for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0dfea1-8880-4a47-bfaf-d829c2b27447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/yzl - awww, tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  @switchfoot http://twitpic.com/yzl - awww, tha...       0\n",
       "1  is upset that he can't update his facebook by ...       0\n",
       "2  @kenichan i dived many times for the ball. man...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  @nationwideclass no, it's not behaving at all....       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_numbers_and_whitespace(text):\n",
    "    # Remove digits\n",
    "    text = ''.join(char for char in text if not char.isdigit())\n",
    "    # Normalize whitespace\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(clean_numbers_and_whitespace)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd9150-2058-47ea-8ed4-d4e0b2248b42",
   "metadata": {},
   "source": [
    "### Replace Mentions, Hashtags and Remove URLs\n",
    "\n",
    "We replace Twitter mentions (e.g., @elonmusk) with a generic token (`usermention`), remove URLs which typically do not carry sentiment information and introduce noise, and strip hashtags (e.g., `#happy` → `happy`) so the keyword remains useful for modeling. This helps clean the data while preserving tweet structure and retaining emotionally relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac7135d-4ecd-4c49-9538-eea9d023e4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9ac7135d-4ecd-4c49-9538-eea9d023e4bb",
    "outputId": "2c4d70ea-88d0-4c7b-aa1e-f697fe202b40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention   - a that's a bummer. you shoulda...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention  i dived many times for the ball. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention  no, it's not behaving at all. i'm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  usermention   - a that's a bummer. you shoulda...       0\n",
       "1  is upset that he can't update his facebook by ...       0\n",
       "2  usermention  i dived many times for the ball. ...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  usermention  no, it's not behaving at all. i'm...       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all @mentions with \"usermention\"\n",
    "tweets['text'] = tweets['text'].str.replace(r'@\\w+', 'usermention ', regex=True)\n",
    "\n",
    "# Remove '#' from hashtags but keep the word\n",
    "tweets['text'] = tweets['text'].str.replace(r'#', '', regex=True)\n",
    "\n",
    "# Remove URLs\n",
    "tweets['text'] = tweets['text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bdc16-f4d2-496e-90ba-4a84f92ed399",
   "metadata": {},
   "source": [
    "#### Why We Keep the 'usermention' Token\n",
    "\n",
    "Instead of removing user mentions entirely (e.g., `@elonmusk`), we replace them with a generic placeholder token: `usermention`. This preserves the fact that the tweet is directed at someone, which can carry important sentiment signals.\n",
    "\n",
    "For example:\n",
    "- `\"usermention you are amazing!\"` likely expresses positive sentiment toward someone\n",
    "- `\"usermention this is terrible\"` may reflect negative sentiment or a complaint\n",
    "\n",
    "By standardizing all mentions to a single token:\n",
    "- We remove identity-specific noise (e.g., usernames)\n",
    "- We retain structural and emotional intent, especially useful for models like **Logistic Regression**, **LSTM**, and **BERT**, which can learn patterns involving direct address\n",
    "\n",
    "This token helps the model distinguish between general statements and targeted expressions — often a subtle but meaningful difference in sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb785e-356b-4e5e-9739-18316ce1bf67",
   "metadata": {},
   "source": [
    "### Punctuation & Special Character Removal\n",
    "\n",
    "We remove standard punctuation symbols (e.g., `.`, `,`, `!`, `?`) to reduce vocabulary noise and standardize tokens. This step helps models focus on the core semantic content of tweets rather than stylistic or formatting variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f00dbfa-a027-4ab9-91c0-f138f5efe9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention    a thats a bummer you shoulda go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention  i dived many times for the ball m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention  no its not behaving at all im mad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  usermention    a thats a bummer you shoulda go...       0\n",
       "1  is upset that he cant update his facebook by t...       0\n",
       "2  usermention  i dived many times for the ball m...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  usermention  no its not behaving at all im mad...       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_pattern = r'[{}]'.format(re.escape(string.punctuation))\n",
    "\n",
    "# Remove punctuation\n",
    "tweets['text'] = tweets['text'].str.replace(punctuation_pattern, '', regex=True)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3deee-d28d-441b-98b2-80dc892ec571",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Before removing stopwords, we tokenize each tweet into individual words. This allows us to filter out unwanted tokens and prepare the data for model ingestion. We use `nltk.word_tokenize()` for consistent, linguistically aware token boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242d6085-c498-4e10-a2df-e44211e48b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].str.replace(r'@\\w+', 'usermention ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e219e5a6-4e7d-4aa6-9f92-f9cd691cc51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention    a thats a bummer you shoulda go...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, a, thats, a, bummer, you, should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention  i dived many times for the ball m...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, i, dived, many, times, for, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention  no its not behaving at all im mad...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, no, its, not, behaving, at, all,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  usermention    a thats a bummer you shoulda go...       0   \n",
       "1  is upset that he cant update his facebook by t...       0   \n",
       "2  usermention  i dived many times for the ball m...       0   \n",
       "3     my whole body feels itchy and like its on fire       0   \n",
       "4  usermention  no its not behaving at all im mad...       0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [usermention, a, thats, a, bummer, you, should...  \n",
       "1  [is, upset, that, he, cant, update, his, faceb...  \n",
       "2  [usermention, i, dived, many, times, for, the,...  \n",
       "3  [my, whole, body, feels, itchy, and, like, its...  \n",
       "4  [usermention, no, its, not, behaving, at, all,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Tokenize tweets\n",
    "tweets['tokens'] = tweets['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ce604-5b55-4711-804c-b18def1bfaae",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Stopwords are very common words (like \"the\", \"and\", \"is\") that generally carry little semantic meaning in text classification. Removing them helps focus on words with stronger sentiment signal. We apply this step using a vectorized approach across all tweets to improve efficiency. The stopword list is extended to also remove punctuation and common social media symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195c6cfd-9edf-47fe-a8d2-053350510402",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add extra symbols to stop words\n",
    "stop_words.update(string.punctuation)\n",
    "stop_words.update([\"''\", \"'\", '``', '’', '“', '”','–', '—', '…', '..', '.', ',', ':', ';', '?', '!', '(', ')', '[', ']', '{', '}', '/', '|'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f75667-7ea3-47de-af27-3262138e6aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention thats bummer shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, thats, bummer, shoulda, got, dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>0</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention dived many times ball managed save...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, dived, many, times, ball, manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  usermention thats bummer shoulda got david car...       0   \n",
       "1  upset cant update facebook texting might cry r...       0   \n",
       "2  usermention dived many times ball managed save...       0   \n",
       "3                   whole body feels itchy like fire       0   \n",
       "4               usermention behaving im mad cant see       0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [usermention, thats, bummer, shoulda, got, dav...  \n",
       "1  [upset, cant, update, facebook, texting, might...  \n",
       "2  [usermention, dived, many, times, ball, manage...  \n",
       "3            [whole, body, feels, itchy, like, fire]  \n",
       "4        [usermention, behaving, im, mad, cant, see]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords but keep 'usermention'\n",
    "tweets['tokens'] = tweets['tokens'].apply(\n",
    "    lambda tokens: [word for word in tokens if word.lower() not in stop_words or word == 'usermention']\n",
    ")\n",
    "\n",
    "# Rejoin tokens to text for downstream models\n",
    "tweets['text'] = tweets['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680c8c8-f974-47b0-9e03-2e01771d14b9",
   "metadata": {},
   "source": [
    "#### Why We Extend the Stopword List with Punctuation and Special Symbols\n",
    "\n",
    "Although we remove most punctuation at the raw text level using string replacement, some unwanted characters and symbols can still survive into the tokenized text. This is especially true for:\n",
    "\n",
    "- Unicode punctuation (e.g., smart quotes `“ ”`, apostrophes `’`, ellipses `…`)\n",
    "- Duplicate or broken punctuation (e.g., `..`, `...`)\n",
    "- Special characters not included in `string.punctuation` (e.g., `{}`, `[]`, `|`)\n",
    "- Symbols that may be split as tokens by the tokenizer (e.g., `(`, `)`, `/`, `|`)\n",
    "\n",
    "To ensure we clean these up consistently, we **extend the stopword list** with both:\n",
    "- The built-in `string.punctuation`, and\n",
    "- A custom list of additional noisy tokens that frequently appear in tweets\n",
    "\n",
    "This helps reduce vocabulary noise, avoid irrelevant tokens in our models, and ensures that even after tokenization, our data remains clean and focused on meaningful words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017cb1f-e29e-4e10-9de3-64477705e689",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization converts each word to its base form — e.g., \"running\" → \"run\". This reduces the total number of unique tokens and helps models generalize across word variations without losing semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6418c2a7-1977-49a0-a314-e13b538e1c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention thats bummer shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, thats, bummer, shoulda, got, dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>0</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention dived many time ball managed save ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, dived, many, time, ball, managed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "      <td>[usermention, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  usermention thats bummer shoulda got david car...       0   \n",
       "1  upset cant update facebook texting might cry r...       0   \n",
       "2  usermention dived many time ball managed save ...       0   \n",
       "3                    whole body feel itchy like fire       0   \n",
       "4               usermention behaving im mad cant see       0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [usermention, thats, bummer, shoulda, got, dav...  \n",
       "1  [upset, cant, update, facebook, texting, might...  \n",
       "2  [usermention, dived, many, time, ball, managed...  \n",
       "3             [whole, body, feel, itchy, like, fire]  \n",
       "4        [usermention, behaving, im, mad, cant, see]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizers\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each token except 'usermention'\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word.lower()) if word != 'usermention' else word for word in tokens]\n",
    "\n",
    "# Apply to the 'tokens' column\n",
    "tweets['tokens'] = tweets['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "tweets['tokens'] = tweets['tokens'].apply(\n",
    "    lambda tokens: [word for word in tokens if word.lower() not in stop_words or word == 'usermention'] \n",
    ")\n",
    "\n",
    "# Rejoin back into cleaned text\n",
    "tweets['text'] = tweets['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f39db3-4629-4b19-96fc-f1514b95a21f",
   "metadata": {},
   "source": [
    "#### Why We Use Lemmatization Instead of Stemming\n",
    "\n",
    "Although stemming (e.g., using `PorterStemmer`) is a common text preprocessing step, we chose **lemmatization** here because it retains more accurate and interpretable base forms of words. For example, lemmatization correctly maps:\n",
    "\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"good\"\n",
    "\n",
    "Whereas stemming would incorrectly output:\n",
    "\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"bett\"\n",
    "\n",
    "This distinction is important for our models:\n",
    "\n",
    "- **Logistic Regression (TF-IDF)**: Lemmatization improves generalization while preserving word meaning.\n",
    "- **LSTM**: Sequence models benefit from accurate word forms to capture semantic structure.\n",
    "- **BERT**: Already handles word variations internally (via subword tokenization), but providing cleaner input can still help.\n",
    "\n",
    "Thus, lemmatization is the better choice for our pipeline, and stemming was deliberately excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "_MFt2E_spfxG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MFt2E_spfxG",
    "outputId": "38140ade-3256-4e25-e6f2-098415cf54d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text'].apply(lambda x: pd.isna(x) or str(x).strip() == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03914f21-6cf6-487f-9571-3c34091b0716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usermention thats bummer shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usermention dived many time ball managed save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usermention behaving im mad cant see</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  usermention thats bummer shoulda got david car...       0\n",
       "1  upset cant update facebook texting might cry r...       0\n",
       "2  usermention dived many time ball managed save ...       0\n",
       "3                    whole body feel itchy like fire       0\n",
       "4               usermention behaving im mad cant see       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert back to DataFrame\n",
    "preprocessed_tweets = tweets.loc[:, ['text', 'target']].copy()\n",
    "preprocessed_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lfGayIZ4pjRP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfGayIZ4pjRP",
    "outputId": "961c9b41-1d05-4cae-f481-d5d90b09029e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets['text'].apply(lambda x: pd.isna(x) or str(x).strip() == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U0kTBjcWp3e-",
   "metadata": {
    "id": "U0kTBjcWp3e-"
   },
   "source": [
    "**we have around 500 \"new\" null text cells introduced after removing stop words. they will be deleted later as a first step before sampling training, validation and testing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z2bj2-WBiF9B",
   "metadata": {
    "id": "Z2bj2-WBiF9B"
   },
   "source": [
    "#### Here we view word cloud of tweets for negative and positive marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AT-nJLMniMXw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "AT-nJLMniMXw",
    "outputId": "dec4c460-233a-460b-e20e-b34917aba2ec"
   },
   "outputs": [],
   "source": [
    "# Negative Tweets word cloud\n",
    "\n",
    "# Filter only negative tweets (target == 0)\n",
    "negative_tweets = preprocessed_tweets[preprocessed_tweets['target'] == 0]['text']\n",
    "\n",
    "# Combine all negative tweet text into a single string\n",
    "text_blob = ' '.join(negative_tweets.astype(str))\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Reds',\n",
    "    max_words=200\n",
    ").generate(text_blob)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Negative Tweets\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9B81T2iMBf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "0b9B81T2iMBf",
    "outputId": "8007c94f-b3ea-4e1e-c72d-7c64b28e04bc"
   },
   "outputs": [],
   "source": [
    "# Positive Tweets word cloud\n",
    "\n",
    "# Filter only positive tweets (target == 4)\n",
    "positive_tweets = preprocessed_tweets[preprocessed_tweets['target'] == 4]['text']\n",
    "\n",
    "# Combine all negative tweet text into a single string\n",
    "text_blob = ' '.join(positive_tweets.astype(str))\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    colormap='Greens',\n",
    "    max_words=200\n",
    ").generate(text_blob)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Positive Tweets\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f8414-d432-4984-92da-02f7dfd7f62e",
   "metadata": {
    "id": "863f8414-d432-4984-92da-02f7dfd7f62e",
    "outputId": "e25d4f94-3ce8-4343-cd98-fc03241f6678"
   },
   "outputs": [],
   "source": [
    "# We now save the processed tweets to be used later in model development\n",
    "\n",
    "processed_data_dir = \"../processed_data\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "output_processed_file_path = os.path.join(processed_data_dir, \"preprocessed_tweets.csv\")\n",
    "preprocessed_tweets.to_csv(output_processed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed tweets saved to: {output_processed_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NfmIEPPkkeyd",
   "metadata": {
    "id": "NfmIEPPkkeyd"
   },
   "source": [
    "#### Next we take first 70% of each negative and positive tweets for training, 15% for validation, and 15% for testing.\n",
    "Sampling sould be done in a way that respects tweets length distribution.\n",
    "\n",
    "This way we make sure all models are trained and validated using the same dataset and we have a good ground for fair comparison.\n",
    "\n",
    "This step is valid because we showed in EDA section that negative and positive tewwts are equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8da74-abb8-45b1-a637-e57386b02a33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "2ba8da74-abb8-45b1-a637-e57386b02a33",
    "outputId": "1958ebd1-63d1-4a9b-d9df-101de692ef62"
   },
   "outputs": [],
   "source": [
    "# Here, we begin by loading the processed dataset and assign column names\n",
    "\n",
    "preprocessed_path = \"../processed_data/preprocessed_tweets.csv\"\n",
    "Tweets = pd.read_csv(preprocessed_path, header=None, names=[\"text\", \"target\"])\n",
    "\n",
    "# Read the preprocessed tweet dataset and assign column names\n",
    "# Tweets = pd.read_csv(\"../data/preprocessed_tweets.csv\", header=None, names=[\"text\", \"target\"])\n",
    "\n",
    "# Drop any rows where 'text' or 'target' is null\n",
    "Tweets = Tweets.dropna(subset=[\"text\", \"target\"])\n",
    "\n",
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fXfKl95mkCwK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXfKl95mkCwK",
    "outputId": "44d72cb5-5093-4b82-b0d9-f279387cdb11"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels from the bar chart in our EDA\n",
    "bin_edges = [0, 20, 40, 60, 80, 100, 120, 140]\n",
    "bin_labels = ['0–20', '21–40', '41–60', '61–80', '81–100', '101–120', '121–140']\n",
    "\n",
    "# Step 1: Assign length buckets safely\n",
    "def assign_length_buckets(df):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['text'].fillna('').astype(str)  # Ensure no NaNs\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['length_bucket'] = pd.cut(\n",
    "        df['text_length'],\n",
    "        bins=bin_edges,\n",
    "        labels=bin_labels,\n",
    "        right=True,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    df = df.dropna(subset=['length_bucket'])  # Drop any that didn't fall in bin\n",
    "    return df\n",
    "\n",
    "# Step 2: Stratified split function\n",
    "def stratified_split_by_length_bucket(df_class, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    df_class = assign_length_buckets(df_class)\n",
    "\n",
    "    # Split into train and temp\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df_class,\n",
    "        test_size=1 - train_ratio,\n",
    "        stratify=df_class['length_bucket'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Split temp into validation and test\n",
    "    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=1 - val_ratio_adjusted,\n",
    "        stratify=temp_df['length_bucket'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return train_df.drop(columns='length_bucket'), val_df.drop(columns='length_bucket'), test_df.drop(columns='length_bucket')\n",
    "\n",
    "# Step 3: Filter by sentiment\n",
    "df_neg = Tweets[Tweets['target'] == 0]\n",
    "df_pos = Tweets[Tweets['target'] == 4]\n",
    "\n",
    "# Step 4: Apply stratified splitting\n",
    "neg_train, neg_val, neg_test = stratified_split_by_length_bucket(df_neg)\n",
    "pos_train, pos_val, pos_test = stratified_split_by_length_bucket(df_pos)\n",
    "\n",
    "# Step 5: Combine and shuffle\n",
    "train_df = pd.concat([neg_train, pos_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = pd.concat([neg_val, pos_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat([neg_test, pos_test]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Extract input and labels\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['target']\n",
    "X_val = val_df['text']\n",
    "y_val = val_df['target']\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['target']\n",
    "\n",
    "# Final check\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"X_train shape:\", X_train.shape, \"| y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape, \"| y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape, \"| y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xR0QU6tft5l2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "xR0QU6tft5l2",
    "outputId": "a943eb95-be38-4b20-e057-bbb4e19a64d8"
   },
   "outputs": [],
   "source": [
    "# here we validate the distributions are identical and intact\n",
    "\n",
    "# Combine text + label for plotting, and calculate text length\n",
    "train_plot_df = pd.concat([X_train, y_train], axis=1).copy()\n",
    "val_plot_df = pd.concat([X_val, y_val], axis=1).copy()\n",
    "test_plot_df = pd.concat([X_test, y_test], axis=1).copy()\n",
    "\n",
    "# Ensure text_length column is present\n",
    "train_plot_df['text_length'] = train_plot_df['text'].str.len()\n",
    "val_plot_df['text_length'] = val_plot_df['text'].str.len()\n",
    "test_plot_df['text_length'] = test_plot_df['text'].str.len()\n",
    "\n",
    "# Ensure 'target' is integer (not string) for palette mapping\n",
    "train_plot_df['target'] = train_plot_df['target'].astype(int)\n",
    "val_plot_df['target'] = val_plot_df['target'].astype(int)\n",
    "test_plot_df['target'] = test_plot_df['target'].astype(int)\n",
    "\n",
    "# Define sentiment color palette\n",
    "sentiment_palette = {\n",
    "    0: '#fc8d62',  # Negative\n",
    "    4: '#66c2a5'   # Positive\n",
    "}\n",
    "\n",
    "# Set up subplot layout\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Plot train\n",
    "sns.violinplot(ax=axes[0], data=train_plot_df, x='target', y='text_length', hue='target', palette=sentiment_palette, inner='quartile', legend=False)\n",
    "axes[0].set_title('Train Set')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels(['Negative (0)', 'Positive (4)'])\n",
    "\n",
    "# Plot validation\n",
    "sns.violinplot(ax=axes[1], data=val_plot_df, x='target', y='text_length', hue='target', palette=sentiment_palette, inner='quartile', legend=False)\n",
    "axes[1].set_title('Validation Set')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels(['Negative (0)', 'Positive (4)'])\n",
    "\n",
    "# Plot test\n",
    "sns.violinplot(ax=axes[2], data=test_plot_df, x='target', y='text_length', hue='target', palette=sentiment_palette, inner='quartile', legend=False)\n",
    "axes[2].set_title('Test Set')\n",
    "axes[2].set_xlabel('Sentiment')\n",
    "axes[2].set_xticks([0, 1])\n",
    "axes[2].set_xticklabels(['Negative (0)', 'Positive (4)'])\n",
    "\n",
    "# Shared Y-label and layout\n",
    "fig.supylabel('Tweet Length (characters)', fontsize=12)\n",
    "plt.suptitle(\"Tweet Length Distribution by Sentiment Across Splits\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rzg0gE-8vxA6",
   "metadata": {
    "id": "Rzg0gE-8vxA6"
   },
   "source": [
    "After we made sure we have good and valid splits, we now move on to modeling part, where we will only use training and validation set for model comparison and selection, ***testing dataset will only be used in model performance evaluation section***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gIWg8bWFMyjE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "gIWg8bWFMyjE",
    "outputId": "30c6a7e4-a548-460f-a7a4-1c707925434a"
   },
   "outputs": [],
   "source": [
    "# Finally, we save splits as CSV to ensure no data leakage takes place as differnt team members split and train the models. this way everyone uses the same training dataset.\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Set this to True if running in Google Colab\n",
    "USE_COLAB = False\n",
    "\n",
    "# Define save path\n",
    "processed_data_dir = \"../processed_data\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "train_path = os.path.join(processed_data_dir, \"train_dataset.csv\")\n",
    "val_path = os.path.join(processed_data_dir, \"val_dataset.csv\")\n",
    "test_path = os.path.join(processed_data_dir, \"test_dataset.csv\")\n",
    "\n",
    "# Save split datasets\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "print(\"Local files saved.\")\n",
    "\n",
    "# Optional: Save as zip archive\n",
    "if USE_COLAB:\n",
    "    zip_path = os.path.join(processed_data_dir, \"sentiment140_splits.zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(train_path, arcname=\"train_dataset.csv\")\n",
    "        zipf.write(val_path, arcname=\"val_dataset.csv\")\n",
    "        zipf.write(test_path, arcname=\"test_dataset.csv\")\n",
    "\n",
    "    print(f\"✅ Datasets saved to: {processed_data_dir}\")\n",
    "    print(f\"📦 Zipped archive saved to: {zip_path}\")\n",
    "\n",
    "# Optional: download in Colab\n",
    "if USE_COLAB:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(train_path)\n",
    "        files.download(val_path)\n",
    "        files.download(test_path)\n",
    "        files.download(zip_path)\n",
    "    except ImportError:\n",
    "        print(\"Google Colab download skipped — not running in Colab.\")\n",
    "\n",
    "    print(\"\\n📦 Ready to commit the archive to your GitHub repository:\")\n",
    "    print(f\"cd to repo folder and run:\\n\")\n",
    "    print(f\"git add {zip_path}\")\n",
    "    print(f'git commit -m \"Add compressed dataset splits for reproducibility\"')\n",
    "    print(\"git push\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
